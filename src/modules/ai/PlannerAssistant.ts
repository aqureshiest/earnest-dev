import { getPlanPrompt } from "../utilities/prompts";
import { OpenAIService } from "./OpenAIService";
import { LLM_MODELS } from "../utilities/llmInfo";
import { ClaudeAIService } from "./ClaudeAIService";
import { TokenLimiter } from "./TokenLimiter";

// create PlanResonse interface
interface PlanResponse {
    plan: string;
    calculatedTokens: number;
    inputTokens: number;
    outputTokens: number;
    cost: number;
}

export class PlannerAssistant {
    constructor() {}

    async executePlan(model: string, task: string, files: FileDetails[]): Promise<PlanResponse> {
        console.log(`Generating plan for task: ${task} with model: ${model}`);

        // retrieve the plan prompt
        const prompt = getPlanPrompt(task);

        // enforce model token limit
        const { totalTokens, allowedFiles } = TokenLimiter.applyTokenLimit(model, prompt, files);

        // construct final prompt with allowed files
        // construct final prompt with allowed files
        const allowedFilesContent = allowedFiles
            .map((file) => `File: ${file.path}\n${file.content}`)
            .join("\n---\n");
        const finalPrompt = prompt.replace("[[EXISTING_CODE_FILES]]", allowedFilesContent);
        // console.log("Final prompt:", finalPrompt);
        console.log("Total tokens:", totalTokens);

        // pick the ai model
        const aiService =
            model === LLM_MODELS.OPENAI_GPT_4O || LLM_MODELS.OPENAI_GPT_4O_MINI
                ? new OpenAIService()
                : new ClaudeAIService();

        // generate plan
        const {
            response: plan,
            inputTokens,
            outputTokens,
            cost,
        } = await aiService.generateResponse(finalPrompt);
        console.log("\n------------------ ------------------ ------------------\n\n");
        console.log(">>> Plan generated by Planner Assistant >>>>:", plan);
        return { plan, calculatedTokens: totalTokens, inputTokens, outputTokens, cost };
    }
}
