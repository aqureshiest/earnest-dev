import { getModelParamsAsync } from "gpt-tokenizer/modelParams";
import { LLM_MODELS } from "../utilities/llmInfo";
import { ClaudeAIService } from "./ClaudeAIService";
import { OpenAIService } from "./OpenAIService";
import { TokenLimiter } from "./TokenLimiter";
import { parseYaml } from "../utilities/parseYaml";

abstract class BaseAssistant<T> implements AIAssistant<T> {
    abstract getSystemPrompt(): string;

    abstract getPrompt(params?: any): string;

    async process(request: AIAssistantRequest): Promise<AIAssistantResponse<T> | null> {
        const { model, task, files, params } = request;
        this.log(`Processing task:\n>>${task}\n>>with model: ${model}`);

        const systemPrompt = this.getSystemPrompt();
        const basePrompt = this.getPrompt();

        // enforce model token limit
        const { totalTokens, allowedFiles } = TokenLimiter.applyTokenLimit(
            model,
            systemPrompt + basePrompt,
            files
        );

        // construct final prompt with allowed files
        const allowedFilesContent = allowedFiles
            .map((file) => `File: ${file.path}\n${file.content}`)
            .join("\n---");

        // create final params
        const extendedParams = {
            ...params,
            taskDescription: task,
            existingCodeFiles: allowedFilesContent,
        };

        // interpolate all keys in the params
        const finalPrompt = Object.keys(extendedParams).reduce((acc, key) => {
            return acc.replace(`[[${key.toUpperCase()}]]`, extendedParams[key]);
        }, basePrompt);

        this.log(finalPrompt);

        // generate response
        const aiResponse = await this.generateResponse(model, systemPrompt, finalPrompt);
        if (!aiResponse) {
            return null;
        }

        // parse the response to specifications
        const parsed = parseYaml(aiResponse.response) as T;
        // console.log(JSON.stringify(parsed, null, 2));

        return {
            ...aiResponse,
            response: parsed,
            calculatedTokens: totalTokens,
        };
    }

    protected log(message: string): void {
        console.log(`[${this.constructor.name}] ${message}`);
    }

    protected async generateResponse(
        model: string,
        systemPrompt: string,
        prompt: string
    ): Promise<AIAssistantResponse<string> | null> {
        // pick the ai model
        const aiService = new ClaudeAIService();

        // generate code
        const { response, inputTokens, outputTokens, cost } = await aiService.generateResponse(
            systemPrompt,
            prompt
        );

        console.log("\n------------------ ------------------ ------------------\n\n");
        console.log(`>>> Response generated by ${this.constructor.name} >>>>:`, response);

        return {
            response,
            calculatedTokens: 0,
            inputTokens,
            outputTokens,
            cost,
        };
    }
}

export { BaseAssistant };
